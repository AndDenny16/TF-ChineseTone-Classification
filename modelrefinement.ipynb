{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Goal: TF Model to Recognize Chinese Tones\n",
    "\n",
    "Author: Andrew Denny\n",
    "\n",
    "Credit for Dataset : [TonePerfect](https://tone.lib.msu.edu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline of My Process:\n",
    "\n",
    "1. Visualize Spectrograms of a few Individual Recordings \n",
    "2. Create Spectrograms for All, and create TF Dataset\n",
    "2. Use Forvo API to pull in some real world examples to help the model generalize better (after some experimentation, realized this would be useful)\n",
    "3. Combine Datasets\n",
    "4. Create Model \n",
    "5. View the Results\n",
    "6. Practice Quantizing Model with TFLite (For practice putting a model on AWS Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import librosa \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"data/tone_perfect/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Many Experiments, Settled on these Librosa functions:\n",
    "- librosa_load - samples audio file at 16 kHZ\n",
    "- melspectrogram - computes the audio files Mel Spectrogram\n",
    "- power_to_db - convert to decibel scale, which is how humans hear tone\n",
    "- specshow  - displays\n",
    "\n",
    "Matplotlib assists with getting rid of axes and resizing to 225x225. Finally, it displays the plot using plt.show(), making it useful for audio feature visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our Plotting Function \n",
    "def librosa_plot(file, hop_length=16, n_fft=2048, fmin=50, fmax=350, nmels=64): \n",
    "    y, sr = librosa.load(file, sr=16000)\n",
    "\n",
    "    # Create mel-spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "        y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=nmels, fmin= fmin, fmax=fmax)\n",
    "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    plt.figure(figsize=(2.25, 2.25))  # Adjust figure size for 225x225\n",
    "\n",
    "    plt.axis('off')  # Remove axes\n",
    "    #Librosa Display Specshow functions\n",
    "    librosa.display.specshow(mel_spectrogram_db, sr=sr, hop_length=hop_length, x_axis=None, y_axis=None)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize 1st Tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsttone1 =  \"data/tone_perfect/bai1_MV3_MP3.mp3\"\n",
    "librosa_plot(firsttone1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa_plot('data/tone_perfect/fan1_FV1_MP3.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see first tone results in a very smooth straight line. Male vs. Female voice results in a small diference of the height of the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Second Tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa_plot('data/tone_perfect/bao2_FV3_MP3.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa_plot(\"data/tone_perfect/hai2_MV1_MP3.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Tone is a sharp increase in tone towards the end of the character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing 3rd Tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa_plot('data/tone_perfect/fou3_FV2_MP3.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this one\n",
    "librosa_plot(\"data/tone_perfect/dao3_MV2_MP3.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3rd tone is the lowest of the 4, results in a deep valley towards the middle of the character. Decrease, then rise again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing 4th Tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "librosa_plot(\"data/tone_perfect/dai4_FV2_MP3.mp3\")\n",
    "librosa_plot(\"data/tone_perfect/ai4_MV3_MP3.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharp Decrease from High pitch to Low pitch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forvo API\n",
    "\n",
    "Due to the Uniform Nature of the Data, I decided to pull in some real world examples from Forvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "FORVO_KEY = os.getenv(\"FORVO_API\")\n",
    "base_url = f\"https://apifree.forvo.com/key/{FORVO_KEY}/\"\n",
    "FORVO_URL = ( base_url + \"format/xml/action/word-pronunciations/word/{word}/language/zh\") # ENDPOINT FOR CHINESE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUICK SCRIPT TO CALL FORVO API URL \n",
    "\n",
    "Search for common words as specified by the Chinese Index saved in repo as : finalchineseindex.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import uuid\n",
    "\n",
    "def forvo_call(character, tone):\n",
    "    response = requests.get(FORVO_URL.format(word = character))\n",
    "    if response.ok:\n",
    "        root = ET.fromstring(response.content)\n",
    "        # Find all <item> elements and extract the <pathogg> content\n",
    "        path_mp3 = []\n",
    "        for item in root.findall('item'):\n",
    "            mp3 = item.find('pathmp3').text\n",
    "            path_mp3.append(mp3)\n",
    "        for i, mp3_url in enumerate(path_mp3):\n",
    "            mp3_response = requests.get(mp3_url)\n",
    "            if mp3_response.status_code == 200:\n",
    "                file_name = f\"data/forvo/{tone}_{uuid.uuid4()}.mp3\"\n",
    "                with open(file_name, \"wb\") as f:\n",
    "                    f.write(mp3_response.content)\n",
    "            else:\n",
    "                print(\"Failed MP3 Fetch\")\n",
    "\n",
    "    else:\n",
    "        print(\"Forvo/Character Not Found\")\n",
    "\n",
    "def get_character_audios():\n",
    "    #this index contains the most common chinese characters\n",
    "    index = pd.read_excel(\"finalchineseindex.xlsx\")\n",
    "    mapping = {'first':1, 'second': 2, 'third': 3, 'fourth': 4}\n",
    "    for (index, row) in index.iterrows():\n",
    "        if index > 400: \n",
    "            break\n",
    "        if row['tone'] == 'neutral' or index > 400: \n",
    "            continue\n",
    "        tone = mapping[row['tone']]\n",
    "        character = row['character']\n",
    "        forvo_call(character, tone)\n",
    "    print(\"Finished Forvo Calls\")\n",
    "\n",
    "get_character_audios()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF DATABASE CREATION\n",
    "\n",
    "- TF Database prefetchs images, ideal for our large 10,000 plus dataset\n",
    "\n",
    "- Need to save our images into 4 folders representing the 4 different tones under 1 subdirectory\n",
    "\n",
    "- File Structure: \n",
    "    tone folder \n",
    "    - 1\n",
    "    - 2\n",
    "    - 3\n",
    "    - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# More formal version of librosa_plot earlier\n",
    "def audio_to_mel_spectrogram(audio_path,output_dir, img_size=(225, 225), low_freq_range=(50, 350)):\n",
    "    \"\"\"\n",
    "    Convert an audio clip to a mel-spectrogram image within a specific frequency range and saves it\n",
    "    \"\"\"\n",
    "    print(output_dir)\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_path, sr=16000)\n",
    "    y, _ = librosa.effects.trim(y, top_db=8)\n",
    "\n",
    "    # Extract mel-spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "         y=y, sr=sr, n_fft=2048, hop_length=16, n_mels=64, fmin=low_freq_range[0], fmax=low_freq_range[1]\n",
    "    )\n",
    "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    plt.figure(figsize=(2.25, 2.25))  # Adjust figure size for 225x225\n",
    "    plt.axis('off')  # Remove axes\n",
    "    librosa.display.specshow(mel_spectrogram_db, sr=sr, hop_length=16, x_axis=None, y_axis=None)\n",
    "    plt.tight_layout()\n",
    "    # Save as image\n",
    "    img_path = os.path.join(output_dir, os.path.basename(audio_path).replace('.mp3', '.png'))\n",
    "    plt.savefig(img_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    # Resize the image to (225, 225, 3)\n",
    "    img = Image.open(img_path).resize(img_size)\n",
    "    img.save(img_path)\n",
    "    print(f\"Saved mel-spectrogram to {img_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will take at least a couple minutes\n",
    "output_directory = (\"data/tone/{tone}\")\n",
    "for f in os.listdir(data_directory):\n",
    "  print(f)\n",
    "  if f.endswith('.mp3'):\n",
    "    output_dir = output_directory.format(tone = f.split(\"_\")[0][-1])\n",
    "    audio_to_mel_spectrogram(os.path.join(data_directory, f), output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF Dataset Creation\n",
    "realdata_path = 'data/tone' #Folder that containers the 4 subfolders\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    realdata_path,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(225, 225),\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation: Using TF Keras\n",
    "\n",
    "- Convolutional Neural Network with 3 Convolution Layers\n",
    "\n",
    "- Softmax with CCE as loss function\n",
    "\n",
    "- 3 Layers of Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Input, Activation, GaussianNoise\n",
    "\n",
    "def get_cnn_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(48, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(120, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization of our Model\n",
    "class_names = dataset.class_names\n",
    "input_shape = (225, 225, 3)\n",
    "classes = len(class_names)\n",
    "model = get_cnn_model(input_shape, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization\n",
    "\n",
    "Without normalization, gradients could explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "#Map our normalization function to the Dataset\n",
    "normalized_dataset = dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "#Test/Train/Validate Split\n",
    "#80/10/10\n",
    "train_size = 0.8 \n",
    "val_size = 0.1\n",
    "\n",
    "dataset_size = len(normalized_dataset)\n",
    "train_dataset = normalized_dataset.take(int(dataset_size * train_size))\n",
    "val_dataset = normalized_dataset.skip(int(dataset_size * train_size)).take(int(dataset_size * val_size))\n",
    "test_dataset = normalized_dataset.skip(int(dataset_size * (train_size + val_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Checkpoint just in case to save our model to a .keras file\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"final.keras\",          \n",
    "    save_best_only=True,               # Save only the best model\n",
    "    monitor='val_accuracy',            # Monitor validation accuracy\n",
    "    mode='max',                        # Save when val_accuracy is maximized\n",
    "    save_weights_only=False,        \n",
    "    verbose=1                          \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, callbacks = [checkpoint], epochs=6, batch_size= 32, verbose=1, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: \n",
    "\n",
    "- During training, the Validation Loss Seemed to plateau and start to decrease after the 4th epoch, so I will use that model checkpoint\n",
    "- Reach 97% Accuracy on the Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"final.keras\")\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "98% Accuracy on the Test Dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Confusion Matrix on Test Dataset\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# 1. Get true labels and predictions\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Iterate through test dataset\n",
    "for images, labels in test_dataset:\n",
    "    y_true.extend(labels.numpy())  # Directly store class indices\n",
    "    predictions = model.predict(images)\n",
    "    y_pred.extend(np.argmax(predictions, axis=1))  # Convert probabilities to class indices\n",
    "\n",
    "# 2. Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "num_classes = cm.shape[0]\n",
    "\n",
    "# 3. Plot confusion matrix using Matplotlib\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(num_classes)\n",
    "plt.xticks(tick_marks, tick_marks)\n",
    "plt.yticks(tick_marks, tick_marks)\n",
    "\n",
    "for i in range(num_classes):\n",
    "    for j in range(num_classes):\n",
    "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", \n",
    "                 color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\")\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Display classification report\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside this Confusion Matrix (all the tones are shifted down 1)\n",
    "\n",
    "- 1st Tone (0) \n",
    "    - 9 False Negative, leading to the lowest recall with 96% \n",
    "    - 6 False Positives\n",
    "    - Most often confused with Second Tone \n",
    "\n",
    "- 2nd Tone (1)\n",
    "    - 9 False Negatives\n",
    "    - 7 False Positives\n",
    "    - 97 % Recall/Precision\n",
    "\n",
    "\n",
    "- 3rd Tone(2)\n",
    "    - 5 False Negatives\n",
    "    - 7 False Positives\n",
    "    - 98% Recall\n",
    "\n",
    "\n",
    "- 4th Tone(3)\n",
    "    - Nearly 100% Recall, with only one value being a False Negative\n",
    "    - Precision and Accuracy Similar\n",
    "    - 5 False Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm happy with these Results, we will try to quantize the model, so we can maybe deploy on AWS Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can this be Quantized For TFLite?  Lets See!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
